---
title: "Example: Geolocating and Cross-Database Matching of US Hydraulic Bridge Collapses"
author: "[Madeleine Flint](http://www.madeleinemflint.com)"
date: "`r Sys.Date()`"
params:
  eval: FALSE
output: 
  html_notebook:
    toc: TRUE
    bibliography: bibliography.bib
---


```{r setup, include=FALSE, eval=params$eval}
library(knitr)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
opts_chunk$set(fig.width=12, fig.height=4, fig.path='Plots/',message=FALSE)
```

```{r directories, include=FALSE, eval=params$eval}
# Hidden: check for consistency with cloned git Repo and for presence of necessary folders and files.
gitRepoName <- "bridge.collapses"
if(grepl(gitRepoName,gsub(paste0("/",gitRepoName),"",getwd()))) warning("Git repository folder name is duplicated, please consider re-cloning.")
if(!grepl(gitRepoName,getwd()))stop("Notebook only functions with working directory set to original repository.")
fold.req <- c("Scripts", "Data")
if (any(!(fold.req %in% list.dirs(path = getwd(), full.names = FALSE, recursive = FALSE)))) stop("Scripts and Data directories and files must be present.")
files.req <- c(paste0(gitRepoName,".Rproj"), "README.md")
if(any(!(files.req %in% list.files()))) stop("Seeming mismatch with directories.")

# check for folders and create analysis folders if missing
fold.req <- c("Plots", "Analysis")
dirsCreate <- fold.req[!(fold.req %in% list.dirs(recursive = FALSE, full.names = FALSE))]
if(length(dirsCreate)) sapply(dirsCreate, function(d) dir.create(d))
fold.req <- c("Processed","Temp","Results","PostProcessed")
dirsCreate <- fold.req[!(fold.req %in% list.dirs(path = "Analysis", recursive = FALSE, full.names = FALSE))]
if(length(dirsCreate)) sapply(dirsCreate, function(d) dir.create(file.path("Analysis",d))) 

# cleanup
rm(fold.req,dirsCreate,gitRepoName,files.req)
```

```{r installLoadPackages, include=FALSE, eval=params$eval}
# Hidden: install and load packages
packages <- c("ggplot2","stringr", "rjson", "knitr", "devtools", "english") 
new.pkg <- packages[!(packages %in% installed.packages())]
if (length(new.pkg)){
    repo <- "http://cran.us.r-project.org"
    install.packages(new.pkg, dependencies = TRUE, repos = repo)
}
invisible(sapply(packages, require, character.only = TRUE))
rm(packages, new.pkg)
```
# Introduction

## Notebook overview
This R Notebook serves as a main "function" for geolocating and augmenting bridge collapse data recorded in the NYSDOT Bridge Failure Database. It supports geolocating through string-matching to bridges in the 2018 US DOT gis version of the National Bridge Inventory (which has verified locations for XXX bridges). It also supports cross-linking to other datasets of interest, including to a dataset of USGS stream gages and the GRanD database of reservoirs and dams. 

## Structure and use of this notebook
The analysis described is broken down into the following steps:

1. *Load* and format input data (from .RData or other file formats)
2. Pre-process and *clean* data (using custom dictionaries)
3. Identify potential *features*  within the fields of each collapse entry (using custom dictionaries and mappings)
4. Perform feature *matching* to applicable target data to obtain a set of candidate matches (either sequentially across features or in parallel)
5. Collate and *rank* the candidate matches (based on user-defined hierarchy of match quality)
6. Support supervised match *confirmation* (interactive)
7. Perform *additional* analyses (e.g., match against additional target data, e.g., USGS gages)

## User controls
The default user controls replicate the analysis described in BLAH. If run interactively, HOW LONG IT WILL TAKE.
```{r userControls, include=TRUE, eval=params$eval}
EXAMPLE_ID   <- c(31, 175) # In West Virginia, STFIPS = 54

# Controls for data use and data storage (memory)
DATA_SETS   <- list(MatchData  = "Fail", # not currently modifiable
                    TargetData = "NBI",  # not currently modifiable
                    Dictionary = c("Stream", "Road", "ITEM5","ITEM43", 
                                   "Cardinal", "Relational", "HydraulicCollapse"),
                    StdData    = c("STFIPS", "FIPS", "GNIS"), # Standard data sets
                    SuppTarget = c("USGS","GRAND"))           # Additional analysis
REDUCE_SIZES <- list(Fail      = c(Example = TRUE,    # only pulls the entry with ID EXAMPLE_ID
                                    Fields  = FALSE), # Extraneous fields deleted?
                     NBI       = c(Fields  = TRUE,   
                                    States  = TRUE)) # Target data of states not present 
                                                     # in match data deleted?


# Controls which parts of Notebook/Analysis are run and data storage (disk)
STEPS <- c("LOAD", "CLEAN", "FEATURE", "MATCH", "RANK", "CONFIRM", "ADD")
RUN   <- c(rep(TRUE, 7))   # Which of the steps above to be run?
SAVE  <- c(rep(FALSE, 7))  # If/when intermediate results to be saved &        
                           # subsequently loaded? Attributes of the saved data 
                           # are used to record data provenance/metadata.
names(RUN)  <- STEPS
names(SAVE) <- STEPS

# Matching options
MATCH_STYLE <- "PAR"       # PAR: Runs for each MATCH_TYPE in parallel (i.e.,
                           # run independently and then combined)
                           # SEQ: Sequential runs (i.e., narrowing down by each match
                           # type in order)?
MATCH_TYPES <- c("road",   # Determines how data should be matched, and supports: 'road',
                 "stream", # 'route', 'stream', and 'bin' (bridge identificatio number). 
                 "route",  # Order in vector determines the prioritization in ranking. 
                 "bin")
MATCH_STORE <- TRUE        # List of potential matches for each entry is stored on disk,
                           # rather than held in memory and returned to the notebook.

# Subfunction controls for messages and BLAH
VERBOSE <- TRUE # Subfunctions to print intermediate results (e.g., # of matches 
                 # completed). Mainly useful for debugging with a small set of MatchData.
```


```{r advanced user controls, include=FALSE, eval=params$eval}
# Features      : which types of data will be cleaned and undergo feature detection
# Match         : controls for string matching and sorting
# maxStringDist : for approximate string matching, max distance that will count as 
#                 "approximately" matched, i.e., number of substitutions, deletions, 
#                 insertions, and BLAH.
# capCand*      : when the list of candidate matches (of the potential match targets) 
#                 grows too long the algorithm will assume that none of the candidates are
#                 sufficiently strong, and will return all potential targets.
#   *N          : if number of candidate matches is > capCandN, revoke candidacy
#   *Pct        : if number of candidate matches is > capCandPct*nTargetRows, revoke 
opts <- list(Fields   = list(Fail = c("MAT", "TYPE", "LOCATION", "STREAM", "BIN", "ID"),
                             NBI  = c("ROUTE","LOCATION","ROAD","STREAM", "BIN", "ID")),
             Features = list(Fail = c("COUNTY", "CITY","LOCATION","ROAD","ROUTE", "BRIDGE","STREAM"),
                             NBI  = NULL),
             Keys     = list(Fail = list(LOCATION = c("Relational"),
                                         ROAD     = c("Road"),
                                         ROUTE    = c("Rte"),
                                         BRIDGE   = c("Bridge"),
                                         STREAM   = c("Stream", "Trib", "Bridge", "Near"))),
             Match    = list(maxStringDist = c(road = 2, stream = 2, route = 1, bin = 3),
                             capCandN      = c(road = 200, stream = 100, route = 100, bin = 200),
                             capCandPct    = c(road = 0.7, stream = 0.7, route = 0.7, bin = 0.7)))
```

```{r controlsChecks, include=FALSE, eval=params$eval}
# Hidden: Check that user control inputs are valid.
if (!is.logical(RUN) | !is.logical(SAVE) | length(RUN)!=7 | length(SAVE)!=7){
  stop('RUN and SAVE must be defined as length-7 logicals')
}
if(RUN["MATCH"] & !(MATCH_STYLE %in% c("SEQ", "PAR"))){
  stop('To perform matching, MATCH_STYLE must be defined as SEQ or PAR')
}
if(RUN["MATCH"] & !(all(MATCH_TYPES %in% c("road", "stream","bin","route")))){
  stop('Only matching by road, route, bin, and stream supported.')
}
```

# 1. Load and format raw or minimally pre-processed input data
## Methods 
A key aspect of the matching algorithm is the use of dictionaries and ontologies to relate entries between databases. The json ontologies were created by the authors, with extensive reference to existing metadata
schema, databases of abbreviations, and knowledge of bridge engineering. For example,
```{bash, echo = FALSE}
cat './Data/Input/Dictionaries/StreamKeys.json'
```

## Code
In a hidden chunk, the `Load.Format.Input.Data` subfunction is used to create match and target dataframes with appropriate data classes and names. Input files in RData, CSV, TAB, XLSX, XLS, or GIS (NBI only) are supported, with packages automatically selected based on the input filename extension:

```{bash, echo = FALSE, comment = ''}
sed -n '46,53p;53q' './Scripts/Load Input/Load.Format.Input.Data.R'
```

```{r loadFormat, include = FALSE, eval=params$eval, error=FALSE, message=FALSE}
fp <- file.path("Data","Input")
inputs<-list(Fail      = c(Fail=file.path(fp,"Fail.Example.RData")),
             NBI       = c(NBI=file.path(fp,"NBI.Example.RData")),
             Dictionary = c(Stream=file.path(fp,"Dictionaries","ls.StreamKeys.RData"),
                            Trib=file.path(fp,"Dictionaries","ls.TribKeys.RData"),
                            Road=file.path(fp,"Dictionaries","ls.RoadKeys.RData"),
                            Rte=file.path(fp,"Dictionaries","ls.RteKeys.RData"),
                            Rail=file.path(fp,"Dictionaries","ls.RailKeys.RData"),
                            Bridge=file.path(fp,"Dictionaries","ls.BridgeKeys.RData"),
                            Cardinal=file.path(fp,"Dictionaries","ls.CardKeys.RData"),
             Relational   = file.path(fp,"Dictionaries","ls.RelationalKeys.RData"),
             Place        = file.path(fp,"Dictionaries","ls.PlaceKeys.RData"),
             Jurisdiction = file.path(fp,"Dictionaries","ls.JurisKeys.RData"),
             County       = file.path(fp,"Dictionaries","ls.CountyKeys.RData"),
             Mat          = file.path(fp,"Dictionaries","ls.MatKeys.RData"),
             Type         = file.path(fp,"Dictionaries","ls.TypeKeys.RData"),
             Sup          = file.path(fp,"Dictionaries","ls.SupKeys.RData"),
             Fail         = file.path(fp,"Dictionaries","ls.Fail.Keys.RData"),
             NBI          = file.path(fp,"Dictionaries","ls.NBI.Keys.RData")),
             Ontologies = c(Fail= file.path(fp,"Ontologies","ls.Fail.Ont.RData"),
                            NBI = file.path(fp,"Ontologies","ls.NBI.Ont.RData"),
                            FailFields = file.path(fp,"Ontologies","ls.Fail.Fields.RData"),
                            NBI.Fields = file.path(fp,"Ontologies","ls.NBI.Fields.RData"),
                            ITEM5B = file.path(fp,"Ontologies","ls.ITEM5B.Ont.RData"),
                            ITEM5BRev = 
                              file.path(fp,"Ontologies","ls.ITEM5B.OntRev.RData"),
                            ITEM43A = file.path(fp,"Ontologies","ls.ITEM43A.Ont.RData"),
                            ITEM43B = file.path(fp,"Ontologies","ls.ITEM43B.Ont.RData"),
                            ITEM43BRev =
                              file.path(fp,"Ontologies","ls.ITEM43B.OntRev.RData"),
                            DOT = file.path(fp,"Ontologies","ls.DOT.Keys.RData")),
             StdData    = c(STFIPS= file.path(fp,"Standard","df.States.RData"),
                            FIPS= file.path(fp,"Standard","df.Counties.RData"),
                            GNIS= file.path(fp,"Standard","df.Cities.RData")),
             SuppTarget = c(USGS="",
                            GRAND=""))
if (RUN["LOAD"]){
# load already-formatted data
invisible(lapply(unlist(inputs[c("Dictionary","Ontologies","StdData")]), load, .GlobalEnv))

source(file.path("Scripts","Load Input","Load.Format.Input.Data.R"))
df.Fail  <- Load.Format.Input.Data(inputs[["Fail"]],
                                     DATA_SET    = "Fail",
                                     REDUCE_SIZE = REDUCE_SIZES[["Fail"]]["Fields"],
                                     VERBOSE     = VERBOSE)
df.NBI <- Load.Format.Input.Data(inputs[["NBI"]],
                                     DATA_SET    = "NBI",
                                     REDUCE_SIZE = REDUCE_SIZES[["NBI"]]["Fields"],
                                     VERBOSE     = VERBOSE)

# remove unneeded rows 
if(REDUCE_SIZES[["Fail"]]["Example"]) df.Fail <- df.Fail[df.Fail$ID %in% EXAMPLE_ID,]
if(REDUCE_SIZES[["NBI"]]["States"]){
  MatchStates <- unique(df.Fail[,paste0('STFIPS')])
  df.NBI  <- df.NBI[df.NBI$STFIPS %in% MatchStates,]
}

rm(fp, Load.Format.Input.Data, MatchStates)

}
```

## Results
The as-loaded example row in the NYSDOT Bridge Failure Database is:

```{r}
print(df.Fail[df.Fail$ID %in% EXAMPLE_ID,])
```

# 2. Pre-process and clean data 
A subfunction `Clean.Fields` is used to process entries in both the match (collapsed bridge) and target (NBI) data, with the types of fields cleaned controlled by `opts.Features`. Pre-processing includes the removal of possible escape characters (e.g., `*` or `&`), extra white space, and leading 0s. Entries are then cleaned of known misspellings and other possibly erroneous information, e.g., transcription errors.

```{r cleanData, include = FALSE, eval=params$eval}
if (RUN["CLEAN"]){
  fp <- file.path("Analysis","Processed")
  source(file.path("Scripts","Process Input","Clean.Fields.R"))
  
  FieldNames <- sapply(opts[["Fields"]][["Fail"]], function(i) ls.Fail.Ont[[i]][1],
                     USE.NAMES = TRUE)
  df.Fail  <- Clean.Fields(df.Fail, NULL,
                             DATA_SET     = "Fail",
                             FieldNames   = FieldNames,
                             VERBOSE      = VERBOSE)
  
  FieldNames <- sapply(opts[["Fields"]][["NBI"]], function(i) ls.NBI.Ont[[i]][1],
                     USE.NAMES = TRUE)[c("BIN","ROUTE")]
  df.NBI  <- Clean.Fields(df.NBI, NULL,
                              DATA_SET     = "NBI",
                              FieldNames  = FieldNames,
                              VERBOSE      = VERBOSE)

  rm(fp, Clean.Fields)
}
```


```{r}
print(df.Fail[df.Fail$ID %in% EXAMPLE_ID,c("LOC","STREAM_UNDER","STREAM_NO", "ROUTE_UNDER",
                                               "BIN_NUM","MATER","TYPE_STRUCT", "SUPPORT", "ITEM43A",
                                               "ITEM43A_ALT",  "ITEM43B", "ITEM43B_ALT")])
```

# 3. Identify features of match and target data
Collapsed bridge "LOCATION" and "FEAT_UND" are searched for features, including county, city, bridge, road, route, and stream names. Other identifying information (e.g., "3.5 miles from") is also identified and isolated. Similar data is obtained from the appropriate fields in the target dataset.
```{R featureDetection, include = FALSE, eval=params$eval}
if(RUN["FEATURE"]){
  source(file.path("Scripts","Process Input","Find.Features.R"))
  
  Features <- sapply(opts[["Features"]][["Fail"]], function(i) ls.Fail.Ont[[i]][1],
                     USE.NAMES = TRUE)
  Features <- sub("LOCATION","LOC",Features)
  Features["STREAM"] <- "STREAM_UNDER"
  df.Fail  <- Find.Features(df.Fail, 
                              Features,
                              VERBOSE = FALSE)
  rm(Find.Features)
}
```

```{r}
print(df.Fail[df.Fail$ID %in% EXAMPLE_ID,c("LOC","STREAM_UNDER","STREAM_NO", "ROUTE_UNDER",
                                               "BIN_NUM","MATER","TYPE_STRUCT", "SUPPORT", "ITEM43A",
                                               "ITEM43A_ALT",  "ITEM43B", "ITEM43B_ALT")])
```

# 4. Structured feature and string matching
Identify NBI bridges that could be linked to failed bridges. Matching style and type of matching controlled by `MATCH_STYLE` and `MATCH_TYPES`. Sequential matching will first search for matches to the first type, and will consider only those possible matching when searching for matches to the second (or later) types. Parallel matching searches for the types separately and then combines the match pools. Currently supported types are "road" "route", "stream", and "bin" (Bridge Identification Number from the NBI). County and city information is automatically searched.
```{r matching, include=TRUE, eval=params$eval, error=FALSE}
if (RUN["MATCH"]){
  # pass all data to main function for matching
  ls.Matches <- list()
  for (i in MATCH_TYPES){
    ls.Matches[[i]] <- Match(df.Fail, df.NBI, i, MATCH_STYLE, STORE = MATCH_STORE,
                             STORE_PATH = fp)
  }
 
  rm(Match.Failed.Briges, fp, files, atts)
}
```

# 5. Automated collating and ranking of potential matches
Use quality of match as well as other information, such as date constructed, date failed, material, and type to rank possible matches.
```{r collateRanking, include=TRUE, eval=params$eval}
if (RUN["RANK"]){
  source(file.path("Scripts","Matching","Rank.Matched.Bridges.R"))

  ls.Matches <- Rank.Matched.Bridges(ls.Matches, df.Fail)
  
  rm(Rank.Matched.Briges)
}
```

# 6. Supervised match confirmation
Shows a series of maps and data entry such that possible "true" matches can be identified.
```{r confirmation, echo=FALSE}
if (RUN["CONFIRMATION"]){
  source(file.path(dirsGit$Scripts,"Matching","Confirm.Matched.Bridges.R"))
  Check   <- all(c("Fail", "ls.Matches") %in% ls())
  if(!Check){
    if(!("Fail" %in% ls())) load(file.path("Analysis","Processed","Match.Information.Failed.Bridges.RData"))
    if(!("ls.Matches" %in% ls())) load(file.path("Analysis","Processed","Possible.Matches.Failed.Bridges.RData"))
  }
  ls.Matches <- Confirm.Matched.Bridges(ls.Matches, df.Fail)
  if (SAVE[7]){
    save(ls.Matches, file = file.path("Analysis","Results","Confirmed.Matches.Failed.Bridges.RData"))
    rm(ls.Matches, df.Fail)
  }
  rm(Confirm.Matched.Briges)
}
```

# 7. Additional data collection and analyses
Currently supports searching for hurricanes, dams, and USGS gauges.

# Notebook metadata
```{r sessionInfo, include=FALSE, echo=TRUE, results='markup'}
devtools::session_info()
```

# References

```{r cleanup, include=FALSE, echo=FALSE, error=FALSE, eval=params$eval}
# Hidden: clear temporary files and environment
if(MATCH_STORE){
  DeleteList <- list.files(path = file.path("Analysis","Temp"), full.names = TRUE)
  file.remove(DeleteList)
}
rm(list = ls())
```

