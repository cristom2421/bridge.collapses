---
title: "Notebook: Geolocating and Cross-Database Matching of US Hydraulic Bridge Collapses"
author: "[Madeleine Flint](http://www.mflint.cee.vt.edu)"
date: "`r Sys.Date()`"
params:
  eval: FALSE
output: 
  html_notebook:
    toc: TRUE
    bibliography: bibliography.bib
---


```{r setup, include=FALSE, eval=params$eval}
library(knitr)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
opts_chunk$set(fig.width=12, fig.height=4, fig.path='Plots/',message=FALSE)
```

```{r directories, include=FALSE, eval=params$eval}
# Hidden: check for consistency with cloned git Repo and for presence of necessary folders and files.
gitRepoName <- "hydraulic.failures"
if(grepl(gitRepoName,gsub(paste0("/",gitRepoName),"",getwd()))) warning("Git repository folder name is duplicated, please consider re-cloning.")
if(!grepl(gitRepoName,getwd()))stop("Notebook only functions with working directory set to original repository.")
fold.req <- c("Scripts", "Data")
if (any(!(fold.req %in% list.dirs(path = getwd(), full.names = FALSE, recursive = FALSE)))) stop("Scripts and Data directories and files must be present.")
files.req <- c(paste0(gitRepoName,".Rproj"), "README.md")
if(any(!(files.req %in% list.files()))) stop("Seeming mismatch with directories.")

# check for folders and create analysis folders if missing
fold.req <- c("Plots", "Analysis")
dirsCreate <- fold.req[!(fold.req %in% list.dirs(recursive = FALSE, full.names = FALSE))]
if(length(dirsCreate)) sapply(dirsCreate, function(d) dir.create(d))
fold.req <- c("Processed","Temp","Results","PostProcessed")
dirsCreate <- fold.req[!(fold.req %in% list.dirs(path = "Analysis", recursive = FALSE, full.names = FALSE))]
if(length(dirsCreate)) sapply(dirsCreate, function(d) dir.create(file.path("Analysis",d))) 

# cleanup
rm(fold.req,dirsCreate,gitRepoName,files.req)
```

```{r installLoadPackages, include=FALSE, eval=params$eval}
# Hidden: install and load packages
packages <- c("ggplot2","stringr", "rjson", "knitr", "devtools", "english") # myround(cor(x,y), 2), broman
new.pkg <- packages[!(packages %in% installed.packages())]
if (length(new.pkg)){
    repo <- "http://cran.us.r-project.org"
    install.packages(new.pkg, dependencies = TRUE, repos = repo)
}
invisible(sapply(packages, require, character.only = TRUE))
rm(packages, new.pkg)
```
# Introduction
## Project overview
Blah

## Notebook overview
This R Notebook serves as a main "function" for geolocating and data-AUGMENTING of bridge collapses recorded in the NYSDOT Bridge Failure Database. It supports geolocating through string-matching to bridges in the 2018 US DOT gis version of the National Bridge Inventory (which has verified locations for XXX bridges). It also supports cross-linking to other datasets of interest, including to a dataset of USGS stream gages and the GRanD database of reservoirs and dams. The motivation and XXX of the Notebook are described in a paper, XXX. Flowcharts, examples, and blah are archived at XXX, with DOI XXXX.

The notebook and supporting scripts and datasets were originally developed (c.2014-2016) for use in locating hydraulic collapses for a study of the return periods of the stream flows causing collapse [ @flint2016]. The human-aided development of custom dictionaries related to abbreviations, misspellings, and XXX are therefore inclusive only of terms present in the 1127 hydraulic collapses present in the 2014 NYSDOT Bridge Failure Database. Additional effort would be required to develop dictionaries for other collapse causes, however, the algorithms (subfunctions) themselves would require only minimal revision to support additional cases.

It is the intent of the author to maintain this Notebook and associated git repository (http://code.vt.edu/mflint/???) in support of the use of the scripts in locating collapses of other causes, and for expansion of the types of datasets available for cross-linking. It is therefore licensed under the ODC-ODbl Open Database License, see http://opendatacommons.org/licenses/odbl/summary/. WRONG LICENSE??? The associated repository supports issue-tracking, feature requests, and XXX.

Suggested citation: XXXX

## Structure and use of this notebook
The analysis described is broken down into the following steps:

1. *Load* and format input data (from .RData or other file formats)
2. Pre-process and *clean* data (using custom dictionaries)
3. Identify potential *features*  within the fields of each collapse entry (using custom dictionaries and mappings)
4. Perform feature *matching* to applicable target data to obtain a set of candidate matches (either sequentially across features or in parallel)
5. Collate and *rank* the candidate matches (based on user-defined hierarchy of match quality)
6. Support supervised match *confirmation* (interactive)
7. Perform *additional* analyses (e.g., match against additional target data, e.g., USGS gages)

If the .Rmd file is run interactively, this notebook is capable of completing the steps, i.e., reproducing the results described in XXX, or of new analyses BLAH. In this case, each step calls a subfunction that will load input data from the previous step and write output data. As long as the output data from prior steps is not deleted it is therefore possible to continue the analysis at a later point in time using the stored data.

If this notebook is being viewed in a knitted form (i.e., html, pdf, or BLAH), then BLAH.

## User controls
The default user controls replicate the analysis described in BLAH. If run interactively, HOW LONG IT WILL TAKE.
```{r userControls, include=TRUE, eval=params$eval}
# Controls for data use and data storage (memory)
DATA_SETS   <- list(MatchData  = "Fail", # not currently modifiable
                    TargetData = "NBI",  # not currently modifiable
                    Dictionary = c("Stream", "Road", "ITEM5","ITEM43", 
                                   "Cardinal", "Relational", "HydraulicCollapse"),
                    StdData    = c("STFIPS", "FIPS", "GNIS"), # Standard data sets
                    SuppTarget = c("USGS","GRAND"))           # Additional analysis
REDUCE_SIZES <- list(MatchData  = c(Debug  = FALSE,    # only pulls first 10 entries
                                    Fields = FALSE), # Extraneous fields deleted?
                     TargetData = c(Fields = TRUE,   
                                    States = FALSE)) # Target data of states not present 
                                                     # in match data deleted?

# Controls which parts of Notebook/Analysis are run and data storage (disk)
STEPS <- c("LOAD", "CLEAN", "FEATURE", "MATCH", "RANK", "CONFIRM", "ADD")
RUN   <- c(rep(TRUE, 7))   # Which of the steps above to be run?
SAVE  <- c(rep(FALSE, 3),  # If/when intermediate results to be saved &        
           TRUE, TRUE,     # subsequently loaded? Attributes of the saved data 
           FALSE, FALSE)   # are used to record data provenance/metadata.
names(RUN)  <- STEPS
names(SAVE) <- STEPS

# Matching options
MATCH_STYLE <- "PAR"       # PAR: Runs for each MATCH_TYPE in parallel (i.e.,
                           # run independently and then combined)
                           # SEQ: Sequential runs (i.e., narrowing down by each match
                           # type in order)?
MATCH_TYPES <- c("road",   # Determines how data should be matched, and supports: 'road',
                 "stream", # 'route', 'stream', and 'bin' (bridge identificatio number). 
                 "route",  # Order in vector determines the prioritization in ranking. 
                 "bin")
MATCH_STORE <- TRUE        # List of potential matches for each entry is stored on disk,
                           # rather than held in memory and returned to the notebook.

# Subfunction controls for messages and BLAH
VERBOSE <- TRUE # Subfunctions to print intermediate results (e.g., # of matches 
                 # completed)? Mainly useful for debugging with a small set of MatchData.
```

Advanced controls to set options for the matching algorithm are also available in the .Rmd (the code chunk is hidden). The predefined defaults should be sufficient for most cases.
```{r advanced user controls, include=FALSE, eval=params$eval}
# Features      : which types of data will be cleaned and undergo feature detection
# Match         : controls for string matching and sorting
# maxStringDist : for approximate string matching, max distance that will count as 
#                 "approximately" matched, i.e., number of substitutions, deletions, 
#                 insertions, and BLAH.
# capCand*      : when the list of candidate matches (of the potential match targets) 
#                 grows too long the algorithm will assume that none of the candidates are
#                 sufficiently strong, and will return all potential targets.
#   *N          : if number of candidate matches is > capCandN, revoke candidacy
#   *Pct        : if number of candidate matches is > capCandPct*nTargetRows, revoke 
opts <- list(Fields   = list(Fail = c("MAT", "TYPE", "LOCATION", "STREAM", "BIN", "ID"),
                             NBI  = c("ROUTE","LOCATION","ROAD","STREAM", "BIN", "ID")),
             Features = list(Fail = c("COUNTY", "CITY","LOCATION","ROAD","ROUTE", "BRIDGE","STREAM"),
                             NBI  = NULL),
             Keys     = list(Fail = list(LOCATION = c("Relational"),
                                         ROAD     = c("Road"),
                                         ROUTE    = c("Rte"),
                                         BRIDGE   = c("Bridge"),
                                         STREAM   = c("Stream", "Trib", "Bridge", "Near"))),
             Match    = list(maxStringDist = c(road = 2, stream = 2, route = 1, bin = 3),
                             capCandN    = c(road = 200, stream = 100, route = 100, bin = 200),
                             capCandPct    = c(road = 0.7, stream = 0.7, route = 0.7, bin = 0.7)))
```

```{r controlsChecks, include=FALSE, eval=params$eval}
# Hidden: Check that user control inputs are valid.
if (!is.logical(RUN) | !is.logical(SAVE) | length(RUN)!=7 | length(SAVE)!=7){
  stop('RUN and SAVE must be defined as length-7 logicals')
}
if(RUN["MATCH"] & !(MATCH_STYLE %in% c("SEQ", "PAR"))){
  stop('To perform matching, MATCH_STYLE must be defined as SEQ or PAR')
}
if(RUN["MATCH"] & !(all(MATCH_TYPES %in% c("road", "stream","bin","route")))){
  stop('Only matching by road, route, bin, and stream supported.')
}
```

# 1. Load and format raw or minimally pre-processed input data
As the geolocation and BLAH depends heavily on standard BLAH, BLAH.

## Methods 
A key aspect of the matching algorithm is the use of dictionaries and ontologies to relate entries between databases. The json ontologies were created by the authors, with extensive reference to BLAH and BLAH. Scripts where the ontologies can be BLAHed are available BLAH. For example,
```{bash, echo = FALSE}
cat './Data/Input/Dictionaries/StreamKeys.json'
```

DISPLAY LINE OF NBI

## Code
In a hidden chunk, the `Load.Format.Input.Data` subfunction is used to create match and target dataframes with appropriate data classes and names. Input files in RData, CSV, TAB, XLSX, XLS, or GIS (NBI only) are supported. Attributes are used for metadata and recording of data provenance. The user should modify input files if different than given in the list `inputs`.

Other standard data are loaded from the provided RData files. The chunk assumes that the dictionaries and dataframes of US state STFIPS codes, county FIPS codes, and place GNIS codes are already correctly formatted.

On a high-end mid-2010s desktop this chunk takes under 5 minutes to evaluate.
```{r loadFormat, include = FALSE, eval=params$eval, error=FALSE, message=FALSE}
fp <- file.path("Data","Input")
inputs<-list(MatchData  = c(Fail=file.path(fp,"Datasets","FAIL_WIN.csv")),
             TargetData = c(NBI=file.path(fp,"Datasets","National_Bridge_Inventory.csv")),
             Dictionary = c(Stream=file.path(fp,"Dictionaries","ls.StreamKeys.RData"),
                            Trib=file.path(fp,"Dictionaries","ls.TribKeys.RData"),
                            Road=file.path(fp,"Dictionaries","ls.RoadKeys.RData"),
                            Rte=file.path(fp,"Dictionaries","ls.RteKeys.RData"),
                            Rail=file.path(fp,"Dictionaries","ls.RailKeys.RData"),
                            Bridge=file.path(fp,"Dictionaries","ls.BridgeKeys.RData"),
                            Cardinal=file.path(fp,"Dictionaries","ls.CardKeys.RData"),
             Relational   = file.path(fp,"Dictionaries","ls.RelationalKeys.RData"),
             Place        = file.path(fp,"Dictionaries","ls.PlaceKeys.RData"),
             Jurisdiction = file.path(fp,"Dictionaries","ls.JurisKeys.RData"),
             County       = file.path(fp,"Dictionaries","ls.CountyKeys.RData"),
             Mat          = file.path(fp,"Dictionaries","ls.MatKeys.RData"),
             Type         = file.path(fp,"Dictionaries","ls.TypeKeys.RData"),
             Sup          = file.path(fp,"Dictionaries","ls.SupKeys.RData"),
             Fail         = file.path(fp,"Dictionaries","ls.Fail.Keys.RData"),
             NBI         = file.path(fp,"Dictionaries","ls.NBI.Keys.RData")),
             Ontologies = c(Fail= file.path(fp,"Ontologies","ls.Fail.Ont.RData"),
                            NBI = file.path(fp,"Ontologies","ls.NBI.Ont.RData"),
                            FailFields = file.path(fp,"Ontologies","ls.Fail.Fields.RData"),
                            NBI.Fields = file.path(fp,"Ontologies","ls.NBI.Fields.RData"),
                            ITEM5B = file.path(fp,"Ontologies","ls.ITEM5B.Ont.RData"),
                            ITEM5BRev = 
                              file.path(fp,"Ontologies","ls.ITEM5B.OntRev.RData"),
                            ITEM43A = file.path(fp,"Ontologies","ls.ITEM43A.Ont.RData"),
                            ITEM43B = file.path(fp,"Ontologies","ls.ITEM43B.Ont.RData"),
                            ITEM43BRev =
                              file.path(fp,"Ontologies","ls.ITEM43B.OntRev.RData"),
                            DOT = file.path(fp,"Ontologies","ls.DOT.Keys.RData")),
             StdData    = c(STFIPS= file.path(fp,"Standard","df.States.RData"),
                            FIPS= file.path(fp,"Standard","df.Counties.RData"),
                            GNIS= file.path(fp,"Standard","df.Cities.RData")),
             SuppTarget = c(USGS="",
                            GRAND=""))
if (RUN["LOAD"]){
# load already-formatted data
invisible(lapply(unlist(inputs[c("Dictionary","Ontologies","StdData")]), load, .GlobalEnv))

# rename DATA_SET-specific dictionaries and ontologies
ls.MatchFields <- get(paste0("ls.",DATA_SETS[["MatchData"]],".Fields"))
ls.TargetFields <- get(paste0("ls.",DATA_SETS[["TargetData"]],".Fields"))
ls.MatchOnt <- get(paste0("ls.",DATA_SETS[["MatchData"]],".Ont"))
ls.TargetOnt <- get(paste0("ls.",DATA_SETS[["TargetData"]],".Ont"))
ls.MatchKeys <- get(paste0("ls.",DATA_SETS[["MatchData"]],".Keys"))
ls.TargetKeys <- get(paste0("ls.",DATA_SETS[["TargetData"]],".Keys"))

source(file.path("Scripts","Load Input","Load.Format.Input.Data.R"))
MatchData  <- Load.Format.Input.Data(inputs[["MatchData"]],
                                     DATA_SET    = DATA_SETS[["MatchData"]],
                                     REDUCE_SIZE = REDUCE_SIZES[["MatchData"]]["Fields"],
                                     VERBOSE     = VERBOSE)
TargetData <- Load.Format.Input.Data(inputs[["TargetData"]],
                                     DATA_SET    = DATA_SETS[["TargetData"]],
                                     REDUCE_SIZE = REDUCE_SIZES[["TargetData"]]["Fields"],
                                     VERBOSE     = VERBOSE)

# remove unneeded rows and set attributes
if(REDUCE_SIZES[["MatchData"]]["Debug"]) MatchData <- MatchData[1:10,]
if(REDUCE_SIZES[["TargetData"]]["States"]){
  MatchStates <- unique(MatchData[,paste0('STFIPS_',toupper(DATA_SETS[["TargetData"]]))])
  TargetData  <- TargetData[TargetData$STFIPS %in% MatchStates,]
}

attributes(MatchData)[c("DATA_SET", "DATA_FILE","DATA_STATUS","DATE_CREATE")] <- 
                       c(DATA_SETS[["MatchData"]], inputs[["MatchData"]], "formatted",
                         Sys.Date())
attributes(TargetData)[c("DATA_SET", "DATA_FILE","DATA_STATUS","DATE_CREATE")] <- 
                       c(DATA_SETS[["TargetData"]], inputs[["TargetData"]], "formatted",
                         Sys.Date())

if(SAVE["LOAD"]){
  save(MatchData,  file = file.path("Analysis","Processed","Formatted.MatchData.RData"))
  save(TargetData, file = file.path("Analysis","Processed","Formatted.TargetData.RData"))
  rm(MatchData, TargetData)
  }
rm(fp, Load.Format.Input.Data, MatchStates)
rm(list = ls(pattern = paste0("ls.",DATA_SETS[["MatchData"]],"|ls.",DATA_SETS[["TargetData"]])))
}
```

# 2. Pre-process and clean data 
A subfunction `Clean.Fields` is used to process entries in both the match (collapsed bridge) and target (NBI) data, with the types of fields cleaned controlled by `opts.Features`. Pre-processing includes the removal of ossible escape characters (e.g., `*` or `&`), extra white space, and leading 0s. Entries are then cleaned of known misspellings and other possibly erroneous information, e.g., transcription errors.

Four subfunctions are used: `PreProcess.StructureFields`, `PreProcess.Location`, `PreProcess.Stream`, and `PreProcess.Numberic`. The fields for which preprocessing and cleaning are available are:
* `TYPE`: translate "girdfb" and "girder floorbeam" to a standard term for BLAH superstructure types, which is then linked to the NBI ontology for structure types. Any identified support conditions (i.e., simple or continuous) are also pulled out, as is any material description.
* `MAT`: translate "prestress" and "pconc" (or similar) to standard term for prestressed concrete, then augment with available support data and link to the NBI ontology for structural materials (which requires support condition for steel, concrete, and prestressed concrete materials).
* `LOCATION`: fix misspellings and known abbreviations for place names (e.g., counties and cities), as well as bridge names. Correction of county names is specific to the state when ambiguous (e.g., Webb versus Webster counties).
* `STREAM`: extract any numeric data associated with a stream crossed by the bridge. For example, "16-mi creek" would be replaced by "sixteen mile creek" to aid later feature detection and matching, whereas "drainage ditch (20)" would have "20" moved to a new `STREAM_NO` field.
* `BIN`: remove letters and standardize format, e.g., "01ad84098" would become "1 84098"
* `ROUTE`: remove letters and standardize format, e.g., "0005.1" would become "5.1"

EXAMPLE

```{r cleanData, include = FALSE, eval=params$eval}
if (RUN["CLEAN"]){
  fp <- file.path("Analysis","Processed")
  source(file.path("Scripts","Process Input","Clean.Fields.R"))
  if (RUN["LOAD"] & SAVE["LOAD"]& !("MatchData" %in% ls())){
    load(file.path(fp,"Formatted.MatchData.RData"))
    load(file.path(fp,"Formatted.TargetData.RData"))
  }
  
  FieldNames <- sapply(opts[["Fields"]][[DATA_SETS[["MatchData"]]]], function(i) ls.MatchOnt[[i]][1],
                     USE.NAMES = TRUE)
  MatchData  <- Clean.Fields(MatchData, NULL,
                             DATA_SET     = DATA_SETS[["MatchData"]],
                             FieldNames     = FieldNames,
                             VERBOSE      = VERBOSE)
  
  FieldNames <- sapply(opts[["Fields"]][[DATA_SETS[["TargetData"]]]], function(i) ls.TargetOnt[[i]][1],
                     USE.NAMES = TRUE)
  TargetData  <- Clean.Fields(TargetData, NULL,
                              DATA_SET     = DATA_SETS[["TargetData"]],
                              FieldNames     = FieldNames,
                              VERBOSE      = VERBOSE)
  
attributes(MatchData)[c("DATA_SET", "DATA_FILE","DATA_STATUS","DATE_CREATE")] <- 
                       c(DATA_SETS[["MatchData"]], inputs[["MatchData"]], "cleaned",
                         Sys.Date())
attributes(TargetData)[c("DATA_SET", "DATA_FILE","DATA_STATUS","DATE_CREATE")] <- 
                       c(DATA_SETS[["TargetData"]], inputs[["TargetData"]], "cleaned",
                         Sys.Date())
  if (SAVE["CLEAN"]){
    save(MatchData,  file = file.path("Analysis","Processed","Cleaned.MatchData.RData"))
    save(TargetData, file = file.path("Analysis","Processed","Cleaned.TargetData.RData"))
    rm(MatchData, TargetData)
  }
  rm(fp, Clean.Fields)
}
```

# 3. Identify features of match and target data
Collapsed bridge "LOCATION" and "FEAT_UND" are searched for features, including county, city, bridge, road, route, and stream names. Other identifying information (e.g., "3.5 miles from") is also identified and isolated. Similar data is obtained from the appropriate fields in the target dataset.
```{R featureDetection, include = FALSE, eval=params$eval}
if(RUN["FEATURE"]){
  source(file.path(dirsGit$Scripts,"Process Input","Find.Features.R"))
  if(RUN["CLEAN"] & SAVE["CLEAN"] & !("MatchData" %in% ls())){ 
    load(file.path("Analysis","Processed","Cleaned.MatchData.RData"))
    load(file.path("Analysis","Processed","Cleaned.TargetData.RData"))
  }
  if(!("MatchData" %in% ls())) stop("Cleaned match & target data not in environment.")
  if(attributes(MatchData)$DATA_STATUS != "cleaned"){
    stop("Cleaned match & target data not in environment.")}
  Features <- sapply(opts[["Features"]][[DATA_SETS[["MatchData"]]]], function(i) ls.MatchOnt[[i]][1],
                     USE.NAMES = TRUE)
  Features <- sub("LOCATION","LOC",Features)
  Features["STREAM"] <- "STREAM_UNDER"
  MatchData  <- Find.Features(MatchData, 
                              Features,
                              VERBOSE = VERBOSE)
  TargetData <- Find.Features(TargetData, opts.Features[DATA_SETS[["TargetData"]]],
                              VERBOSE = VERBOSE)
  
  attributes(MatchData)[c("DATA_SET", "DATA_FILE","DATA_STATUS","DATE_CREATE")] <- 
                       c(DATA_SETS[["MatchData"]], inputs[["MatchData"]], "features",
                         Sys.Date())
  attributes(TargetData)[c("DATA_SET", "DATA_FILE","DATA_STATUS","DATE_CREATE")] <- 
                       c(DATA_SETS[["TargetData"]], inputs[["TargetData"]], "features", 
                         Sys.Date())
  
  if (SAVE["FEATURE"]){
    save(MatchData, file = file.path("Analysis","Processed","Features.MatchData.RData"))
    save(TargetData,  file = file.path("Analysis","Processed","Features.TargetData.RData"))
    rm(MatchData,TargetData)
  }
  rm(Find.Features)
}
```

# 4. Structured feature and string matching
Identify NBI bridges that could be linked to failed bridges. Matching style and type of matching controlled by `MATCH_STYLE` and `MATCH_TYPES`. Sequential matching will first search for matches to the first type, and will consider only those possible matching when searching for matches to the second (or later) types. Parallel matching searches for the types separately and then combines the match pools. Currently supported types are "road" "route", "stream", and "bin" (Bridge Identification Number from the NBI). County and city information is automatically searched.
```{r matching, include=TRUE, eval=params$eval, error=FALSE}
if (RUN["MATCH"]){
  source(file.path("Scripts","Matching","Match.R"))
  fp <- file.path("Analysis","Processed","Temp")
  if(RUN["CLEAN"] & SAVE["CLEAN"] & !("MatchData" %in% ls())){
    load(file.path("Analysis","Processed","Features.MatchData.RData"))
    load(file.path("Analysis","Processed","Features.TargetData.RData"))
  }
  if(!("MatchData" %in% ls())) stop("Featured match & target data not in environment.")
  if(attributes(MatchData)$DATA_STATUS != "features"){
    stop("Featured match & target data not in environment.")}
  
  # pass all data to main function for matching
  ls.Matches <- list()
  for (i in MATCH_TYPES){
    ls.Matches[[i]] <- Match(MatchData, TargetData, i, MATCH_STYLE, STORE = MATCH_STORE,
                             STORE_PATH = fp)
  }
  if(MATCH_STORE){ # if matches are not returned to notebook, create metadata file & load
    for (i in MATCH_TYPES){
      files <- list.files(path=file.path(fp,i), pattern = "json", full.names = TRUE)
      atts <- list(DATA_SET_MATCH = DATA_SETS[["MatchData"]], DATA_SET_TARGET = 
                     DATA_SETS[["TargetData"]], MATCH_TYPES = MATCH_TYPES, MATCH_STYLE = 
                     MATCH_STYLE, DATE_CREATE = Sys.Date())
      cat(toJSON(atts), file = file.path(fp,i))
      ls.Matches[[i]] <- lapply(files, function(f) fromJSON(NULL,f))
      names(ls.Matches[[i]]) <- gsub("[^0-9]","",files)
    }
  }
  
  attributes(ls.Matches)[c("DATA_SET_MATCH","DATA_SET_TARGET","MATCH_TYPES", "MATCH_STYLE",
                           "DATE_CREATE")] <- c(DATA_SETS[["MatchData"]], 
                                                DATA_SETS[["TargetData"]],
                                                MATCH_TYPES, MATCH_STYLE, Sys.Date()) 
  if (SAVE["MATCH"]){
    save(ls.Matches, file = file.path("Analysis","Results","Possible.Matches.RData"))
    rm(ls.Matches, MatchData, TargetData)
  }
  rm(Match.Failed.Briges, fp, files, atts)
}
```

# 5. Automated collating and ranking of potential matches
Use quality of match as well as other information, such as date constructed, date failed, material, and type to rank possible matches.
```{r collateRanking, include=TRUE, eval=params$eval}
if (RUN["RANK"]){
  source(file.path("Scripts","Matching","Rank.Matched.Bridges.R"))

  ls.Matches <- Rank.Matched.Bridges(ls.Matches, MatchData)
  if (SAVE["RANK"]){
    save(ls.Matches, file = file.path("Analysis","Results","Ranked.Matches.Failed.Bridges.RData"))
    rm(ls.Matches, MatchData, ls.Fail.Sorted)
  }
  rm(Rank.Matched.Briges)
}
```

# 6. Supervised match confirmation
Shows a series of maps and data entry such that possible "true" matches can be identified.
```{r confirmation, echo=FALSE}
if (RUN["CONFIRMATION"]){
  source(file.path(dirsGit$Scripts,"Matching","Confirm.Matched.Bridges.R"))
  Check   <- all(c("MatchData", "ls.Matches") %in% ls())
  if(!Check){
    if(!("MatchData" %in% ls())) load(file.path("Analysis","Processed","Match.Information.Failed.Bridges.RData"))
    if(!("ls.Matches" %in% ls())) load(file.path("Analysis","Processed","Possible.Matches.Failed.Bridges.RData"))
  }
  ls.Matches <- Confirm.Matched.Bridges(ls.Matches, MatchData)
  if (SAVE[7]){
    save(ls.Matches, file = file.path("Analysis","Results","Confirmed.Matches.Failed.Bridges.RData"))
    rm(ls.Matches, MatchData)
  }
  rm(Confirm.Matched.Briges)
}
```

# 7. Additional data collection and analyses
Currently supports searching for hurricanes, dams, and USGS gauges.

# Notebook metadata
```{r sessionInfo, include=FALSE, echo=TRUE, results='markup'}
devtools::session_info()
```

# References

```{r cleanup, include=FALSE, echo=FALSE, error=FALSE, eval=params$eval}
# Hidden: clear temporary files and environment
if(MATCH_STORE){
  DeleteList <- list.files(path = file.path("Analysis","Temp"), full.names = TRUE)
  file.remove(DeleteList)
}
rm(list = ls())
```

